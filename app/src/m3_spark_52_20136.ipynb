{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 14px;\">\n",
    "By:\n",
    "\n",
    "- Mohamed Ayman Mohamed Mohamed abo Tammaa\n",
    "    - 52-20136\n",
    "    - mohamed.abotammaa@student.guc.edu.eg\n",
    "    - P02\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "1. Loading the dataset (5%)\n",
    "2. Perform some simple cleaning (30%)\n",
    "    - Column renaming: 10%\n",
    "    - Detect missing: 35%\n",
    "    - Handle missing: 35%\n",
    "    - Check missing : 20%\n",
    "3. Perform some analysis on the dataset (30%)\n",
    "4. Add new columns with feature engineering (15%)\n",
    "5. Encode categorical columns (10%) \n",
    "6. Create a lookup table for encoding only (5%)\n",
    "7. Saving Cleaned dataseta and lookup table (5%)\n",
    "8. ***BONUS**: Saving the output into a postgres database (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that:** You may not need to run the spark containers since pyspark aleady\n",
    "creates a mini server by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# spark : SparkSession = SparkSession.builder.appName(\"m3_spark\").getOrCreate()\n",
    "# sc : SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Datasets/\"\n",
    "ORIGINAL_DATAFILE = \"fintech_data_38_52_20136.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply load the dataset from the parquet format given in the google drive above\n",
    "- Load the dataset.\n",
    "- Preview first 20 rows.\n",
    "- How many partitions is this dataframe split into?\n",
    "- Change partitions to be equal to the number of your logical cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         Customer Id|           Emp Title|Emp Length|Home Ownership|Annual Inc|Annual Inc Joint|Verification Status|Zip Code|Addr State|Avg Cur Bal|Tot Cur Bal|Loan Id|Loan Status|Loan Amount|State|Funded Amount|      Term|Int Rate|Grade|       Issue Date|Pymnt Plan|      Type|           Purpose|         Description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YidceGI4Llx4YzFce...|             ARX LLC|    1 year|      MORTGAGE|   56700.0|            null|           Verified|   371xx|        TN|    34478.0|   379261.0|  72174| Fully Paid|     8500.0|   TN|       8500.0| 36 months|   0.079|    4|    13 April 2013|     false|INDIVIDUAL|debt_consolidation|         Debt Consol|\n",
      "|YiIwXHgwZVx4OTlce...|Senior Project Ma...|   5 years|      MORTGAGE|   88000.0|            null|    Source Verified|   615xx|        IL|     3746.0|    41204.0| 119409| Fully Paid|    12000.0|   IL|      12000.0| 36 months|  0.0999|    9|15 September 2015|     false|Individual|       credit_card|Credit card refin...|\n",
      "|Yicse3JceGJmXHg4O...|     Project Manager|   2 years|      MORTGAGE|   93500.0|            null|       Not Verified|   300xx|        GA|    11256.0|   157582.0| 201121|    Current|    20000.0|   GA|      20000.0| 60 months|  0.1899|   18|   16 August 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|Yid7XHhlMVx4OWFrX...|                null|  < 1 year|          RENT|   38000.0|            null|       Not Verified|   235xx|        VA|     4554.0|    40988.0|  95601|    Current|    10000.0|   VA|      10000.0| 36 months|   0.143|   12|  19 October 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YiJXXHhmN1x4Zjlce...|                null|      null|      MORTGAGE|   60000.0|            null|       Not Verified|   442xx|        OH|     4349.0|    56539.0|  50691|    Current|     6600.0|   OH|       6600.0| 36 months|  0.1308|    7| 19 December 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceGQ0Vlx4MWNce...|          Accountant|   2 years|          RENT|   53000.0|            null|    Source Verified|   981xx|        WA|     2419.0|    41118.0| 120090| Fully Paid|    12000.0|   WA|      12000.0| 36 months|  0.1091|    6|   17 August 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFjXHhiNVx4O...|Federal Officer T...|   9 years|          RENT|   85000.0|            null|       Not Verified|   201xx|        VA|     2582.0|    23241.0| 113335| Fully Paid|    11400.0|   VA|      11400.0| 36 months|  0.1099|    7|    15 March 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YiJceGM0aEJceDhlX...|          supervisor|   2 years|          RENT|   30000.0|            null|    Source Verified|   329xx|        FL|      415.0|     3317.0|  61115| Fully Paid|     7850.0|   FL|       7850.0| 36 months|  0.1786|   18|    15 March 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDg4XHhlZVx4M...|  Physical Therapist| 10+ years|          RENT|  112000.0|            null|       Not Verified|   112xx|        NY|     6634.0|    79602.0| 214661|    Current|    24000.0|   NY|      24000.0| 36 months|  0.0532|    5|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGE4XHgwYi1GX...|                null|      null|      MORTGAGE|   21500.0|            null|           Verified|   320xx|        FL|     6221.0|    62208.0|  38618| Fully Paid|     5925.0|   FL|       5925.0| 36 months|  0.1075|    7| 16 February 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yic5VVx4YTFceGNmX...|  Joining Specialist|   7 years|      MORTGAGE|   70000.0|            null|    Source Verified|   373xx|        TN|    13909.0|    97363.0|  51060|    Current|     6725.0|   TN|       6725.0| 36 months|  0.1106|    8|     18 July 2018|     false|Individual|             other|               Other|\n",
      "|YidkXHgxOFx4Zjhce...|MB Operations Sr ...|   4 years|          RENT|   66500.0|            null|    Source Verified|   750xx|        TX|     2909.0|    40724.0| 244454|    Current|    30000.0|   TX|      30000.0| 60 months|   0.124|    7|19 September 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGRmXHhhYVx4Y...|Field Service Man...|  < 1 year|          RENT|   65000.0|            null|       Not Verified|   787xx|        TX|     3007.0|    21050.0|  84814|    Current|    10000.0|   TX|      10000.0| 36 months|  0.0707|    1|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yidcclx4MDZceGY1X...|    Systems Engineer|   3 years|          RENT|   60000.0|            null|       Not Verified|   064xx|        CT|     2845.0|    36990.0|  25199| Fully Paid|     5000.0|   CT|       5000.0| 36 months|  0.0532|    4|    16 March 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YiJSXHg5MFxcXHg4N...|     Account Manager|   7 years|          RENT|   68000.0|            null|           Verified|   606xx|        IL|     5485.0|    49366.0| 235675| Fully Paid|    28000.0|   IL|      28000.0| 36 months|  0.1147|   10|    16 April 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|YiJceDEwXHg4NDhce...|             manager| 10+ years|          RENT|   70000.0|            null|       Not Verified|   905xx|        CA|     1945.0|    33065.0|  64000|    Current|     8000.0|   CA|       8000.0| 36 months|  0.0899|    6|16 September 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|YidfXHgxMmlceGJlX...|                null|      null|      MORTGAGE|   72000.0|            null|    Source Verified|   021xx|        MA|     4337.0|    56375.0| 193130| Fully Paid|    20000.0|   MA|      20000.0| 36 months|  0.1212|    8| 12 November 2012|     false|INDIVIDUAL|       credit_card|      Lower Interest|\n",
      "|YidRXHg5Mlx4YjRce...|            Mechanic| 10+ years|           ANY|   88000.0|            null|    Source Verified|   490xx|        MI|     7809.0|   132749.0| 140167|    Current|    14000.0|   MI|      14000.0| 60 months|   0.124|    7|    19 April 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidISFx4ZDJceGU1a...|Field Service Tec...| 10+ years|      MORTGAGE|  175000.0|            null|    Source Verified|   767xx|        TX|    13954.0|   209308.0|  68637|Charged Off|     8000.0|   TX|       8000.0| 36 months|    null|   12|   17 August 2017|     false|Individual|             other|               Other|\n",
      "|YiJWXHg4Mlx4MDJce...|Administrative As...| 10+ years|      MORTGAGE|   83000.0|            null|       Not Verified|   900xx|        CA|     6404.0|   179312.0|   6902| Fully Paid|     2500.0|   CA|       2500.0| 36 months|  0.0619|    4|     18 June 2018|     false|Individual|       credit_card|Credit card refin...|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df_raw : DataFrame = spark.read.parquet(data_dir + ORIGINAL_DATAFILE)\n",
    "fintech_df_raw.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of partitions originally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is partioned into '''1''' partition(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset is partioned into '''{fintech_df_raw.rdd.getNumPartitions()}''' partition(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My logical cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of logical cores im my pc: 16\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Number of logical cores im my pc: {logical_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is partioned into '''16''' partition(s)\n"
     ]
    }
   ],
   "source": [
    "fintech_df_raw = fintech_df_raw.repartition(logical_cores)\n",
    "print(f\"The dataset is partioned into '''{fintech_df_raw.rdd.getNumPartitions()}''' partition(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Rename all columns (replacing a space with an underscore, and making it lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- annual_inc_joint: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- avg_cur_bal: double (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- loan_id: long (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_amount: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- funded_amount: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: double (nullable = true)\n",
      " |-- grade: long (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- pymnt_plan: boolean (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_column_names(df : DataFrame) -> DataFrame:\n",
    "    df_cpy = df\n",
    "    for col in df.columns:\n",
    "        df_cpy = df_cpy.withColumnRenamed(col, col.replace(\" \", \"_\").lower())\n",
    "    return df_cpy\n",
    "\n",
    "fintech_df = clean_column_names(fintech_df_raw)\n",
    "fintech_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Detect missing\n",
    "   - Create a function that takes in the df and returns any data structrue of your choice(df/dict,list,tuple,etc) which has the name of the column and percentage of missing entries from the whole dataset.\n",
    "   - Tip : storing the missing info as dict where the key is the column name and value is the percentage would be the easiest.\n",
    "#### - Prinout the missing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annual_inc_joint': 0.9298187199408066, 'emp_title': 0.08812430632630411, 'emp_length': 0.06977432482426933, 'int_rate': 0.04679985201627821, 'description': 0.009174990751017388, 'customer_id': 0.0, 'home_ownership': 0.0, 'annual_inc': 0.0, 'verification_status': 0.0, 'zip_code': 0.0, 'addr_state': 0.0, 'avg_cur_bal': 0.0, 'tot_cur_bal': 0.0, 'loan_id': 0.0, 'loan_status': 0.0, 'loan_amount': 0.0, 'state': 0.0, 'funded_amount': 0.0, 'term': 0.0, 'grade': 0.0, 'issue_date': 0.0, 'pymnt_plan': 0.0, 'type': 0.0, 'purpose': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def get_missing_values(df : DataFrame) -> dict:\n",
    "    missing_values = {}\n",
    "    for col in df.columns:\n",
    "        missing_values[col] = df.filter(fn.col(col).isNull()).count() / df.count()\n",
    "    # sort the dictionary by values (missing values) in descending order\n",
    "    missing_values = dict(sorted(missing_values.items(), key=lambda x: x[1], reverse=True))\n",
    "    return missing_values\n",
    "\n",
    "missing_values = get_missing_values(fintech_df)\n",
    "print(missing_values)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Handle missing\n",
    "- For numerical features replace with 0.\n",
    "- For categorical/strings replace with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp_value(df : DataFrame, col : str, strategy : str = \"mode\") -> any:\n",
    "    if strategy == \"mean\":\n",
    "        imp_value = df.select(fn.mean(col)).collect()[0][0]\n",
    "    else:\n",
    "        # if strategy == \"mode\":\n",
    "        imp_value = df.filter(fn.col(col).isNotNull()).groupBy(col).count()\\\n",
    "            .orderBy(fn.desc(\"count\")).limit(1).select(col).collect()[0][0]\n",
    "    return imp_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def handle_missing_values(df : DataFrame) -> DataFrame:\n",
    "    missing_vals = dict(filter(lambda x: x[1] > 0, get_missing_values(df).items()))\n",
    "\n",
    "    schema = df.schema\n",
    "    for col in missing_vals.keys():\n",
    "        if schema[col].dataType == IntegerType() or schema[col].dataType == DoubleType():\n",
    "            df = df.fillna(subset=[col], value=0)\n",
    "        else:\n",
    "            # if schema[col].dataType == StringType() or schema[col].dataType == BooleanType():\n",
    "            df = df.fillna(subset=[col], value=get_imp_value(df, col, \"mode\"))\n",
    "    return df\n",
    "\n",
    "fintech_df = handle_missing_values(fintech_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Check missing\n",
    "- Afterwards, check that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the dataset after handling missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of missing values in the dataset after handling missing values: \"+ \\\n",
    "    str(len(dict(filter(lambda x: x[1] > 0.0, get_missing_values(fintech_df).items())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode only the following categorical values\n",
    "- Emp Length: Change to numerical\n",
    "- Home Ownership: One Hot Encoding\n",
    "- Verification Status: One Hot Encoding\n",
    "- State: Label Encoding\n",
    "- Type: One Hot Encoding\n",
    "- Purpose: Label Encoding\n",
    "- For the grade, only descretize it to be letter grade, not need to label encode it further\n",
    "\n",
    "**DO NOT** Encode the employment title of description or any other column that is not mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emp_length_to_int(df : DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"emp_length\", fn.regexp_replace(\"emp_length\", \"[^0-9]\", \"\"))\n",
    "    df = df.withColumn(\"emp_length\", fn.col(\"emp_length\").cast(IntegerType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_indexers(df : DataFrame, cols : list) -> list:\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    return [StringIndexer(inputCol=col, outputCol=col+\"_encoded\").fit(df) for col in cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df : DataFrame, cols : list) -> tuple:\n",
    "    indexers = get_string_indexers(df, cols)\n",
    "    for indexer in indexers:\n",
    "        df = indexer.transform(df)\n",
    "    return df, indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df : DataFrame, cols : list) -> DataFrame:\n",
    "    from pyspark.ml.feature import OneHotEncoder, StringIndexerModel\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "    # first, clean the column values (replace spaces and dashes with underscores) and convert them to lowercase\n",
    "    for col in cols:\n",
    "        df = df.withColumn(col, fn.lower(fn.regexp_replace(col, \" \", \"_\")))\\\n",
    "            .withColumn(col, fn.lower(fn.regexp_replace(col, \"-\", \"_\")))\n",
    "\n",
    "\n",
    "    # index and label encode the columns (prerequisite for one-hot encoding) (https://www.skytowner.com/explore/one_hot_encoding_in_pyspark)\n",
    "    df, index_fitters = label_encode(df, cols)\n",
    "    index_fitters : StringIndexerModel = index_fitters\n",
    "\n",
    "    # one-hot encode the columns to vector\n",
    "    encoder = OneHotEncoder(dropLast=False, inputCols=[col + \"_encoded\" for col in cols], outputCols=[col + \"_hencoded\" for col in cols])\n",
    "    df_encoded = encoder.fit(df).transform(df)\n",
    "    df_encoded = df_encoded.drop(*[col + \"_encoded\" for col in cols])\n",
    "    \n",
    "    # convert the vector to array\n",
    "    df_encoded = df_encoded.select(\"*\", *[vector_to_array(col).alias(col+\"_array\") for col in [col + \"_hencoded\" for col in cols]])\n",
    "    df_encoded = df_encoded.drop(*[col + \"_hencoded\" for col in cols])\n",
    "\n",
    "    # expand the array to columns and rename the columns\n",
    "    for col, indexer in zip(cols, index_fitters): \n",
    "        num_categories = len(df_encoded.first()[col + \"_hencoded_array\"]) \n",
    "        labels = indexer.labels \n",
    "        cols_expanded = [fn.col(col + \"_hencoded_array\")[i].alias(f'{col}_{str(labels[i]).lower().replace(\" \", \"_\").replace(\"-\", \"_\")}') for i in range(num_categories)] \n",
    "        df_encoded = df_encoded.select(\"*\", *cols_expanded)\n",
    "\n",
    "    df_encoded = df_encoded.drop(*[col + \"_hencoded_array\" for col in cols])\n",
    "\n",
    "    # Convert the columns to binary (0 and 1) \n",
    "    for col, indexer in zip(cols, index_fitters):\n",
    "        labels = indexer.labels \n",
    "        for i in range(len(labels)):\n",
    "            df_encoded = df_encoded.withColumn(f'{col}_{str(labels[i]).lower().replace(\" \", \"_\").replace(\"-\", \"_\")}', fn.col(f'{col}_{str(labels[i]).lower().replace(\" \", \"_\").replace(\"-\", \"_\")}').cast('boolean'))\n",
    "\n",
    "    return df_encoded   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|grade|grade_discretized|\n",
      "+-----+-----------------+\n",
      "|   21|                E|\n",
      "|   29|                F|\n",
      "|    9|                B|\n",
      "|   35|                G|\n",
      "|    8|                B|\n",
      "|   33|                G|\n",
      "|   17|                D|\n",
      "|   10|                B|\n",
      "+-----+-----------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def discretize_column(df : DataFrame, col : str, bins_limits :list = None, labels:list = None) -> DataFrame:\n",
    "    if bins_limits is None and col == \"grade\":\n",
    "        bins_limits = [5, 10, 15, 20, 25, 30]\n",
    "        labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "    elif labels is None:\n",
    "        labels = [f\"{col}_{i}\" for i in range(len(bins_limits) - 1)]\n",
    "    \n",
    "    df = df.withColumn(col+\"_discretized\", fn.when(fn.col(col) <= bins_limits[0], labels[0]).otherwise(fn.col(col)))\n",
    "    for i in range(1, len(bins_limits)):\n",
    "        df = df.withColumn(col+\"_discretized\", fn.when(fn.col(col+\"_discretized\") <= bins_limits[i], labels[i]).otherwise(fn.col(col+\"_discretized\")))\n",
    "    df = df.withColumn(col+\"_discretized\", fn.when(fn.col(col+\"_discretized\") > bins_limits[-1], labels[-1]).otherwise(fn.col(col+\"_discretized\")))\n",
    "\n",
    "    return df\n",
    "discretize_column(fintech_df, \"grade\").select(\"grade\", \"grade_discretized\").distinct().show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, apply the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Emp Length: Change to numerical\n",
    "# - Home Ownership: One Hot Encoding\n",
    "# - Verification Status: One Hot Encoding\n",
    "# - State: Label Encoding\n",
    "# - Type: One Hot Encoding\n",
    "# - Purpose: Label Encoding\n",
    "# - For the grade, only descretize it to be letter grade, not need to label encode it further\n",
    "\n",
    "fintech_df = convert_emp_length_to_int(fintech_df)\n",
    "fintech_df = one_hot_encode(fintech_df, [\"home_ownership\", \"verification_status\", \"type\"])\n",
    "fintech_df = label_encode(fintech_df, [\"state\", \"purpose\"])[0]\n",
    "fintech_df = discretize_column(fintech_df, \"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+----------+------------------+--------------------+-----------------------+-------------------+------------------+------------------+--------------------+-----------------------------------+--------------------------------+----------------------------+---------------+--------------+----------+---------------+-------------+---------------+-----------------+\n",
      "|         customer_id|      emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|      issue_date|pymnt_plan|      type|           purpose|         description|home_ownership_mortgage|home_ownership_rent|home_ownership_own|home_ownership_any|home_ownership_other|verification_status_source_verified|verification_status_not_verified|verification_status_verified|type_individual|type_joint_app|type_joint|type_direct_pay|state_encoded|purpose_encoded|grade_discretized|\n",
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+----------+------------------+--------------------+-----------------------+-------------------+------------------+------------------+--------------------+-----------------------------------+--------------------------------+----------------------------+---------------+--------------+----------+---------------+-------------+---------------+-----------------+\n",
      "|YidceGJiS1x4OGN3X...|Account Manager|         1|      mortgage|   79000.0|             0.0|       not_verified|   950xx|        CA|    55198.0|   551977.0| 182410|    Current|    18475.0|   CA|      18475.0| 36 months|  0.0796|    4|   18 March 2018|     false|individual|       credit_card|Credit card refin...|                   true|              false|             false|             false|               false|                              false|                            true|                       false|           true|         false|     false|          false|          0.0|            1.0|                A|\n",
      "|Yic6XHg5ND5ceDEzY...|        Teacher|        10|           own|   71000.0|             0.0|    source_verified|   700xx|        LA|     2120.0|    21195.0|  28829|    Current|     5000.0|   LA|       5000.0| 36 months|  0.1075|    6|16 February 2016|     false|individual|debt_consolidation|  Debt consolidation|                  false|              false|              true|             false|               false|                               true|                           false|                       false|           true|         false|     false|          false|         26.0|            0.0|                B|\n",
      "|YiJceDFjXHg4NjZce...| Tax Specialist|        10|          rent|   60000.0|             0.0|    source_verified|   983xx|        WA|     2028.0|    20279.0|  64324|    Current|     8000.0|   WA|       8000.0| 36 months|  0.0949|   10|    16 July 2016|     false|individual|debt_consolidation|  Debt consolidation|                  false|               true|             false|             false|               false|                               true|                           false|                       false|           true|         false|     false|          false|         15.0|            0.0|                B|\n",
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+----------+------------------+--------------------+-----------------------+-------------------+------------------+------------------+--------------------+-----------------------------------+--------------------------------+----------------------------+---------------+--------------+----------+---------------+-------------+---------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that adds the 3 following features. Try as much as you can to use built in fucntions in PySpark (from the functions library) check lab 8.\n",
    "<br> Avoid writing UDFs from scratch.\n",
    "- Previous loan issue date from the same grade\n",
    "- Previoius Loan amount from the same grade\n",
    "- Previous loan date from the same state and grade combined\n",
    "- Previous loan amount from the same state and grade combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert the issue date to a date format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_issueD_to_date(df : DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"issue_date\", fn.to_date(fn.to_timestamp(fn.col(\"issue_date\"), \"d MMMM yyyy\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_new_features(df : DataFrame) -> DataFrame:\n",
    "    #first convert the issue_date to date for proper date sorting\n",
    "    df = convert_issueD_to_date(df)\n",
    "\n",
    "    # Add prv_grade_loan_issue_date\n",
    "    window = Window.partitionBy(\"grade\").orderBy(\"issue_date\")\n",
    "    df = df.withColumn(\"prv_grade_loan_issue_date\", fn.lag(\"issue_date\").over(window))\n",
    "\n",
    "    # Add prv_loan_amnt\n",
    "    #TODO: check if this is correct\n",
    "    df = df.withColumn(\"prv_loan_amount\", fn.lag(\"loan_amount\").over(window))\n",
    "\n",
    "    # Add prv_state_grade_issue_date\n",
    "    window = Window.partitionBy(\"state\", \"grade\").orderBy(\"issue_date\")\n",
    "    df = df.withColumn(\"prv_state_grade_issue_date\", fn.lag(\"issue_date\").over(window))\n",
    "\n",
    "    # Add prv_state_grade_loan_amount\n",
    "    df = df.withColumn(\"prv_state_grade_loan_amount\", fn.lag(\"loan_amount\").over(window))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------------+-----------+---------------+-------------------------+\n",
      "|issue_date|grade|grade_discretized|loan_amount|prv_loan_amount|prv_grade_loan_issue_date|\n",
      "+----------+-----+-----------------+-----------+---------------+-------------------------+\n",
      "|2013-01-13|   30|                F|    20000.0|           null|                     null|\n",
      "|2013-02-13|   30|                F|     5000.0|        20000.0|               2013-01-13|\n",
      "|2013-04-13|   30|                F|    35000.0|         5000.0|               2013-02-13|\n",
      "|2013-05-13|   30|                F|    21600.0|        35000.0|               2013-04-13|\n",
      "|2013-05-13|   30|                F|    28000.0|        21600.0|               2013-05-13|\n",
      "|2013-06-13|   30|                F|    35000.0|        28000.0|               2013-05-13|\n",
      "|2013-06-13|   30|                F|    22000.0|        35000.0|               2013-06-13|\n",
      "|2013-08-13|   30|                F|    20000.0|        22000.0|               2013-06-13|\n",
      "|2013-08-13|   30|                F|    10000.0|        20000.0|               2013-08-13|\n",
      "|2013-08-13|   30|                F|    29000.0|        10000.0|               2013-08-13|\n",
      "+----------+-----+-----------------+-----------+---------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+--------------------+-----------------------+-------------------+------------------+------------------+--------------------+-----------------------------------+--------------------------------+----------------------------+---------------+--------------+----------+---------------+-------------+---------------+-----------------+-------------------------+---------------+--------------------------+---------------------------+\n",
      "|         customer_id|           emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|issue_date|pymnt_plan|      type|           purpose|         description|home_ownership_mortgage|home_ownership_rent|home_ownership_own|home_ownership_any|home_ownership_other|verification_status_source_verified|verification_status_not_verified|verification_status_verified|type_individual|type_joint_app|type_joint|type_direct_pay|state_encoded|purpose_encoded|grade_discretized|prv_grade_loan_issue_date|prv_loan_amount|prv_state_grade_issue_date|prv_state_grade_loan_amount|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+--------------------+-----------------------+-------------------+------------------+------------------+--------------------+-----------------------------------+--------------------------------+----------------------------+---------------+--------------+----------+---------------+-------------+---------------+-----------------+-------------------------+---------------+--------------------------+---------------------------+\n",
      "|YiJceDkwXHg5N1x4Y...|Heavy Equipment M...|         5|      mortgage|   80000.0|             0.0|       not_verified|   995xx|        AK|    21457.0|   343307.0| 155398|Charged Off|    15000.0|   AK|      15000.0| 60 months|  0.1144|    6|2015-01-15|     false|individual|       credit_card|Credit card refin...|                   true|              false|             false|             false|               false|                              false|                            true|                       false|           true|         false|     false|          false|         44.0|            1.0|                B|               2015-01-15|         9000.0|                      null|                       null|\n",
      "|YidceGQ5XHhkNlx4Z...|Human Resource Co...|         2|          rent|   56640.0|             0.0|    source_verified|   998xx|        AK|     8633.0|   120855.0| 164448| Fully Paid|    16000.0|   AK|      16000.0| 36 months|  0.1153|    6|2015-10-15|     false|individual|debt_consolidation|  Debt consolidation|                  false|               true|             false|             false|               false|                               true|                           false|                       false|           true|         false|     false|          false|         44.0|            0.0|                B|               2015-10-15|        24000.0|                2015-01-15|                    15000.0|\n",
      "|YidceDE1XHg4Zlx4Z...|               Pilot|         1|          rent|   98600.0|             0.0|    source_verified|   995xx|        AK|     3501.0|    21006.0| 262700|    Current|    36000.0|   AK|      36000.0| 36 months|  0.1198|    6|2018-02-18|     false|individual|             house|         Home buying|                  false|               true|             false|             false|               false|                               true|                           false|                       false|           true|         false|     false|          false|         44.0|            8.0|                B|               2018-02-18|        40000.0|                2015-10-15|                    16000.0|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+--------------------+-----------------------+-------------------+------------------+------------------+--------------------+-----------------------------------+--------------------------------+----------------------------+---------------+--------------+----------+---------------+-------------+---------------+-----------------+-------------------------+---------------+--------------------------+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df = add_new_features(fintech_df)\n",
    "fintech_df.filter(fn.col(\"grade\") == 30)\\\n",
    "    .select(\"issue_date\", \"grade\", \"grade_discretized\", \"loan_amount\", \"prv_loan_amount\", \"prv_grade_loan_issue_date\")\\\n",
    "        .orderBy(\"issue_date\").show(10)\n",
    "fintech_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Analysis SQL VS Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer each of the following questions using both SQL and Spark:\n",
    "1. Identify the average loan amount and interest rate for loans marked as \"Default\" in the Loan Status, grouped by Emp Length and annual income ranges.<br>\n",
    "Hint: Use SQL Cases to bin Annual Income into Income Ranges\n",
    "2. Calculate the average difference between Loan Amount and Funded Amount for each\n",
    "loan Grade and sort by the grades with the largest differences.\n",
    "3. Compare the total Loan Amount for loans with \"Verified\" and \"Not Verified\"\n",
    "Verification Status across each state (Addr State).\n",
    "4. Calculate the average time gap (in days) between consecutive loans for each\n",
    "grade using the new features you added in the feature engineering phase.\n",
    "5. Identify the average difference in loan amounts between consecutive loans\n",
    "within the same state and grade combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Lookup Table & Saving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.1: Lookup Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a lookup table for the encodings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.2: Saving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally load (save) the cleaned PySpark df and the lookup table to parquet\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Bonus - Loading to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the cleaned parquet file and lookup table into a Postgres database.\n",
    "- Take Screenshots showing the newly added features in the feature engineering section\n",
    "- Take a screenshot from the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "1. Python Notebook with the following naming m3_spark_<id>.ipynb eg.\n",
    "m3_spark_52_XXXX.ipynb\n",
    "2. Cleaned Parquet file named: fintech_spark_52_XXXX_clean.parquet\n",
    "3. Lookup table named: lookup_spark_52_XXXX.parquet\n",
    "4. Incase of doing the bonus: Screenshots from PGAdmin showing the cleaned table\n",
    "(some of the rows) and another one showing the lookup table.\n",
    "Note: All these files should reside in a folder for milestone 3, inside the root drive\n",
    "folder created previously in milestone 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission guidelines\n",
    "Upload all the deliverables in your google drive milestone folder.\n",
    "Best of luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing Spark Session Context\n",
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DE Env",
   "language": "python",
   "name": "deenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
