{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 14px;\">\n",
    "By:\n",
    "\n",
    "- Mohamed Ayman Mohamed Mohamed abo Tammaa\n",
    "    - 52-20136\n",
    "    - mohamed.abotammaa@student.guc.edu.eg\n",
    "    - P02\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "1. Loading the dataset (5%)\n",
    "2. Perform some simple cleaning (30%)\n",
    "    - Column renaming: 10%\n",
    "    - Detect missing: 35%\n",
    "    - Handle missing: 35%\n",
    "    - Check missing : 20%\n",
    "3. Perform some analysis on the dataset (30%)\n",
    "4. Add new columns with feature engineering (15%)\n",
    "5. Encode categorical columns (10%) \n",
    "6. Create a lookup table for encoding only (5%)\n",
    "7. Saving Cleaned dataseta and lookup table (5%)\n",
    "8. ***BONUS**: Saving the output into a postgres database (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that:** You may not need to run the spark containers since pyspark aleady\n",
    "creates a mini server by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark : SparkSession\n",
    "sc : SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/27 03:25:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark : SparkSession = SparkSession.builder.appName(\"m3_spark\").getOrCreate()\n",
    "sc : SparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Datasets/\"\n",
    "ORIGINAL_DATAFILE = \"fintech_data_38_52_20136.parquet\"\n",
    "CLEAN_DATAFILE = \"fintech_spark_52_20136_clean.parquet\"\n",
    "LOOKUP_FILE = \"lookup_spark_52_20136.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "lookup_schema = StructType([\n",
    "    StructField(\"column\", StringType(), False),\n",
    "    StructField(\"original\", StringType(), True),\n",
    "    StructField(\"imputed\", StringType(), False)\n",
    "])\n",
    "\n",
    "Lookup_Table : DataFrame = spark.createDataFrame(schema=lookup_schema, data=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply load the dataset from the parquet format given in the google drive above\n",
    "- Load the dataset.\n",
    "- Preview first 20 rows.\n",
    "- How many partitions is this dataframe split into?\n",
    "- Change partitions to be equal to the number of your logical cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         Customer Id|           Emp Title|Emp Length|Home Ownership|Annual Inc|Annual Inc Joint|Verification Status|Zip Code|Addr State|Avg Cur Bal|Tot Cur Bal|Loan Id|Loan Status|Loan Amount|State|Funded Amount|      Term|Int Rate|Grade|       Issue Date|Pymnt Plan|      Type|           Purpose|         Description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YidceGI4Llx4YzFce...|             ARX LLC|    1 year|      MORTGAGE|   56700.0|            null|           Verified|   371xx|        TN|    34478.0|   379261.0|  72174| Fully Paid|     8500.0|   TN|       8500.0| 36 months|   0.079|    4|    13 April 2013|     false|INDIVIDUAL|debt_consolidation|         Debt Consol|\n",
      "|YiIwXHgwZVx4OTlce...|Senior Project Ma...|   5 years|      MORTGAGE|   88000.0|            null|    Source Verified|   615xx|        IL|     3746.0|    41204.0| 119409| Fully Paid|    12000.0|   IL|      12000.0| 36 months|  0.0999|    9|15 September 2015|     false|Individual|       credit_card|Credit card refin...|\n",
      "|Yicse3JceGJmXHg4O...|     Project Manager|   2 years|      MORTGAGE|   93500.0|            null|       Not Verified|   300xx|        GA|    11256.0|   157582.0| 201121|    Current|    20000.0|   GA|      20000.0| 60 months|  0.1899|   18|   16 August 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|Yid7XHhlMVx4OWFrX...|                null|  < 1 year|          RENT|   38000.0|            null|       Not Verified|   235xx|        VA|     4554.0|    40988.0|  95601|    Current|    10000.0|   VA|      10000.0| 36 months|   0.143|   12|  19 October 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YiJXXHhmN1x4Zjlce...|                null|      null|      MORTGAGE|   60000.0|            null|       Not Verified|   442xx|        OH|     4349.0|    56539.0|  50691|    Current|     6600.0|   OH|       6600.0| 36 months|  0.1308|    7| 19 December 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceGQ0Vlx4MWNce...|          Accountant|   2 years|          RENT|   53000.0|            null|    Source Verified|   981xx|        WA|     2419.0|    41118.0| 120090| Fully Paid|    12000.0|   WA|      12000.0| 36 months|  0.1091|    6|   17 August 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFjXHhiNVx4O...|Federal Officer T...|   9 years|          RENT|   85000.0|            null|       Not Verified|   201xx|        VA|     2582.0|    23241.0| 113335| Fully Paid|    11400.0|   VA|      11400.0| 36 months|  0.1099|    7|    15 March 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YiJceGM0aEJceDhlX...|          supervisor|   2 years|          RENT|   30000.0|            null|    Source Verified|   329xx|        FL|      415.0|     3317.0|  61115| Fully Paid|     7850.0|   FL|       7850.0| 36 months|  0.1786|   18|    15 March 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceDg4XHhlZVx4M...|  Physical Therapist| 10+ years|          RENT|  112000.0|            null|       Not Verified|   112xx|        NY|     6634.0|    79602.0| 214661|    Current|    24000.0|   NY|      24000.0| 36 months|  0.0532|    5|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGE4XHgwYi1GX...|                null|      null|      MORTGAGE|   21500.0|            null|           Verified|   320xx|        FL|     6221.0|    62208.0|  38618| Fully Paid|     5925.0|   FL|       5925.0| 36 months|  0.1075|    7| 16 February 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|Yic5VVx4YTFceGNmX...|  Joining Specialist|   7 years|      MORTGAGE|   70000.0|            null|    Source Verified|   373xx|        TN|    13909.0|    97363.0|  51060|    Current|     6725.0|   TN|       6725.0| 36 months|  0.1106|    8|     18 July 2018|     false|Individual|             other|               Other|\n",
      "|YidkXHgxOFx4Zjhce...|MB Operations Sr ...|   4 years|          RENT|   66500.0|            null|    Source Verified|   750xx|        TX|     2909.0|    40724.0| 244454|    Current|    30000.0|   TX|      30000.0| 60 months|   0.124|    7|19 September 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGRmXHhhYVx4Y...|Field Service Man...|  < 1 year|          RENT|   65000.0|            null|       Not Verified|   787xx|        TX|     3007.0|    21050.0|  84814|    Current|    10000.0|   TX|      10000.0| 36 months|  0.0707|    1|     17 July 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|Yidcclx4MDZceGY1X...|    Systems Engineer|   3 years|          RENT|   60000.0|            null|       Not Verified|   064xx|        CT|     2845.0|    36990.0|  25199| Fully Paid|     5000.0|   CT|       5000.0| 36 months|  0.0532|    4|    16 March 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YiJSXHg5MFxcXHg4N...|     Account Manager|   7 years|          RENT|   68000.0|            null|           Verified|   606xx|        IL|     5485.0|    49366.0| 235675| Fully Paid|    28000.0|   IL|      28000.0| 36 months|  0.1147|   10|    16 April 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|YiJceDEwXHg4NDhce...|             manager| 10+ years|          RENT|   70000.0|            null|       Not Verified|   905xx|        CA|     1945.0|    33065.0|  64000|    Current|     8000.0|   CA|       8000.0| 36 months|  0.0899|    6|16 September 2016|     false|INDIVIDUAL|       credit_card|Credit card refin...|\n",
      "|YidfXHgxMmlceGJlX...|                null|      null|      MORTGAGE|   72000.0|            null|    Source Verified|   021xx|        MA|     4337.0|    56375.0| 193130| Fully Paid|    20000.0|   MA|      20000.0| 36 months|  0.1212|    8| 12 November 2012|     false|INDIVIDUAL|       credit_card|      Lower Interest|\n",
      "|YidRXHg5Mlx4YjRce...|            Mechanic| 10+ years|           ANY|   88000.0|            null|    Source Verified|   490xx|        MI|     7809.0|   132749.0| 140167|    Current|    14000.0|   MI|      14000.0| 60 months|   0.124|    7|    19 April 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidISFx4ZDJceGU1a...|Field Service Tec...| 10+ years|      MORTGAGE|  175000.0|            null|    Source Verified|   767xx|        TX|    13954.0|   209308.0|  68637|Charged Off|     8000.0|   TX|       8000.0| 36 months|    null|   12|   17 August 2017|     false|Individual|             other|               Other|\n",
      "|YiJWXHg4Mlx4MDJce...|Administrative As...| 10+ years|      MORTGAGE|   83000.0|            null|       Not Verified|   900xx|        CA|     6404.0|   179312.0|   6902| Fully Paid|     2500.0|   CA|       2500.0| 36 months|  0.0619|    4|     18 June 2018|     false|Individual|       credit_card|Credit card refin...|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df_raw : DataFrame = spark.read.parquet(data_dir + ORIGINAL_DATAFILE)\n",
    "fintech_df_raw.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of partitions originally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is partioned into '''1''' partition(s)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset is partioned into '''{fintech_df_raw.rdd.getNumPartitions()}''' partition(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My logical cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of logical cores im my pc: 16\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "logical_cores = psutil.cpu_count(logical=True)\n",
    "print(f\"Number of logical cores im my pc: {logical_cores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repartitioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is partioned into '''16''' partition(s)\n"
     ]
    }
   ],
   "source": [
    "fintech_df_raw = fintech_df_raw.repartition(logical_cores)\n",
    "print(f\"The dataset is partioned into '''{fintech_df_raw.rdd.getNumPartitions()}''' partition(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Rename all columns (replacing a space with an underscore, and making it lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- annual_inc_joint: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- avg_cur_bal: double (nullable = true)\n",
      " |-- tot_cur_bal: double (nullable = true)\n",
      " |-- loan_id: long (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_amount: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- funded_amount: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: double (nullable = true)\n",
      " |-- grade: long (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- pymnt_plan: boolean (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_column_names(df : DataFrame) -> DataFrame:\n",
    "    df_cpy = df\n",
    "    for col in df.columns:\n",
    "        df_cpy = df_cpy.withColumnRenamed(col, col.replace(\" \", \"_\").lower())\n",
    "    return df_cpy\n",
    "\n",
    "fintech_df = clean_column_names(fintech_df_raw)\n",
    "fintech_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Detect missing\n",
    "   - Create a function that takes in the df and returns any data structrue of your choice(df/dict,list,tuple,etc) which has the name of the column and percentage of missing entries from the whole dataset.\n",
    "   - Tip : storing the missing info as dict where the key is the column name and value is the percentage would be the easiest.\n",
    "#### - Prinout the missing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annual_inc_joint': 0.9298187199408066, 'emp_title': 0.08812430632630411, 'emp_length': 0.06977432482426933, 'int_rate': 0.04679985201627821, 'description': 0.009174990751017388, 'customer_id': 0.0, 'home_ownership': 0.0, 'annual_inc': 0.0, 'verification_status': 0.0, 'zip_code': 0.0, 'addr_state': 0.0, 'avg_cur_bal': 0.0, 'tot_cur_bal': 0.0, 'loan_id': 0.0, 'loan_status': 0.0, 'loan_amount': 0.0, 'state': 0.0, 'funded_amount': 0.0, 'term': 0.0, 'grade': 0.0, 'issue_date': 0.0, 'pymnt_plan': 0.0, 'type': 0.0, 'purpose': 0.0}\n"
     ]
    }
   ],
   "source": [
    "def get_missing_values(df : DataFrame) -> dict:\n",
    "    missing_values = {}\n",
    "    for col in df.columns:\n",
    "        missing_values[col] = df.filter(fn.col(col).isNull()).count() / df.count()\n",
    "    # sort the dictionary by values (missing values) in descending order\n",
    "    missing_values = dict(sorted(missing_values.items(), key=lambda x: x[1], reverse=True))\n",
    "    return missing_values\n",
    "\n",
    "missing_values = get_missing_values(fintech_df)\n",
    "print(missing_values)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Handle missing\n",
    "- For numerical features replace with 0.\n",
    "- For categorical/strings replace with mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp_value(df : DataFrame, col : str, strategy : str = \"mode\") -> any:\n",
    "    if strategy == \"mean\":\n",
    "        imp_value = df.select(fn.mean(col)).collect()[0][0]\n",
    "    else:\n",
    "        # if strategy == \"mode\":\n",
    "        imp_value = df.filter(fn.col(col).isNotNull()).groupBy(col).count()\\\n",
    "            .orderBy(fn.desc(\"count\")).limit(1).select(col).collect()[0][0]\n",
    "    return imp_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def handle_missing_values(df : DataFrame) -> DataFrame:\n",
    "    missing_vals = dict(filter(lambda x: x[1] > 0, get_missing_values(df).items()))\n",
    "\n",
    "    schema = df.schema\n",
    "    for col in missing_vals.keys():\n",
    "        if schema[col].dataType == IntegerType() or schema[col].dataType == DoubleType():\n",
    "            df = df.fillna(subset=[col], value=0)\n",
    "        else:\n",
    "            # if schema[col].dataType == StringType() or schema[col].dataType == BooleanType():\n",
    "            df = df.fillna(subset=[col], value=get_imp_value(df, col, \"mode\"))\n",
    "    return df\n",
    "\n",
    "fintech_df = handle_missing_values(fintech_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Check missing\n",
    "- Afterwards, check that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values in the dataset after handling missing values: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of missing values in the dataset after handling missing values: \"+ \\\n",
    "    str(len(dict(filter(lambda x: x[1] > 0.0, get_missing_values(fintech_df).items())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode only the following categorical values\n",
    "- Emp Length: Change to numerical\n",
    "- Home Ownership: One Hot Encoding\n",
    "- Verification Status: One Hot Encoding\n",
    "- State: Label Encoding\n",
    "- Type: One Hot Encoding\n",
    "- Purpose: Label Encoding\n",
    "- For the grade, only descretize it to be letter grade, not need to label encode it further\n",
    "\n",
    "**DO NOT** Encode the employment title of description or any other column that is not mentioned above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emp_length_to_int(df : DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"emp_length\", fn.regexp_replace(\"emp_length\", \"[^0-9]\", \"\"))\n",
    "    df = df.withColumn(\"emp_length\", fn.col(\"emp_length\").cast(IntegerType()))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_indexers(df : DataFrame, cols : list) -> list:\n",
    "    from pyspark.ml.feature import StringIndexer\n",
    "    return [StringIndexer(inputCol=col, outputCol=col+\"_encoded\",stringOrderType=\"alphabetDesc\").fit(df) for col in cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(df : DataFrame, cols : list) -> tuple:\n",
    "    indexers = get_string_indexers(df, cols)\n",
    "    for indexer in indexers:\n",
    "        df = indexer.transform(df)\n",
    "    # convert the encoded columns to integer type\n",
    "    for col in cols:\n",
    "        df = df.withColumn(col+\"_encoded\", fn.col(col+\"_encoded\").cast(IntegerType()))\n",
    "    return df, indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df : DataFrame, cols : list) -> DataFrame:\n",
    "    from pyspark.ml.feature import OneHotEncoder\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "\n",
    "    # first, clean the column values (replace spaces and dashes with underscores) and convert them to lowercase\n",
    "    for col in cols:\n",
    "        df = df.withColumn(col, fn.lower(fn.regexp_replace(col, \" \", \"_\")))\\\n",
    "            .withColumn(col, fn.lower(fn.regexp_replace(col, \"-\", \"_\")))\n",
    "\n",
    "\n",
    "    # index and label encode the columns (prerequisite for one-hot encoding) (https://www.skytowner.com/explore/one_hot_encoding_in_pyspark)\n",
    "    df, index_fitters = label_encode(df, cols)\n",
    "\n",
    "    # one-hot encode the columns to vector\n",
    "    encoder = OneHotEncoder(dropLast=False, inputCols=[col + \"_encoded\" for col in cols], outputCols=[col + \"_hencoded\" for col in cols])\n",
    "    df_encoded = encoder.fit(df).transform(df)\n",
    "    df_encoded = df_encoded.drop(*[col + \"_encoded\" for col in cols])\n",
    "    \n",
    "    # convert the vector to array\n",
    "    df_encoded = df_encoded.select(\"*\", *[vector_to_array(col).alias(col+\"_array\") for col in [col + \"_hencoded\" for col in cols]])\n",
    "    df_encoded = df_encoded.drop(*[col + \"_hencoded\" for col in cols])\n",
    "\n",
    "    # expand the array to columns and rename the columns\n",
    "    for col, indexer in zip(cols, index_fitters): \n",
    "        num_categories = len(df_encoded.first()[col + \"_hencoded_array\"]) \n",
    "        labels = indexer.labels \n",
    "        cols_expanded = [fn.col(col + \"_hencoded_array\")[i].alias(f'{col}_{str(labels[i]).lower().replace(\" \", \"_\").replace(\"-\", \"_\")}') for i in range(num_categories)] \n",
    "        df_encoded = df_encoded.select(\"*\", *cols_expanded)\n",
    "\n",
    "    df_encoded = df_encoded.drop(*[col + \"_hencoded_array\" for col in cols])\n",
    "\n",
    "    # Convert the columns to binary (0 and 1) \n",
    "    for col, indexer in zip(cols, index_fitters):\n",
    "        labels = indexer.labels \n",
    "        for i in range(len(labels)):\n",
    "            df_encoded = df_encoded.withColumn(f'{col}_{str(labels[i]).lower().replace(\" \", \"_\").replace(\"-\", \"_\")}', fn.col(f'{col}_{str(labels[i]).lower().replace(\" \", \"_\").replace(\"-\", \"_\")}').cast('boolean'))\n",
    "\n",
    "    return df_encoded   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|grade|grade_discretized|\n",
      "+-----+-----------------+\n",
      "|   21|                E|\n",
      "|   29|                F|\n",
      "|    9|                B|\n",
      "|   35|                G|\n",
      "|    8|                B|\n",
      "|   33|                G|\n",
      "|   17|                D|\n",
      "|   10|                B|\n",
      "+-----+-----------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def discretize_column(df : DataFrame, col : str, bins_limits :list = None, labels:list = None) -> DataFrame:\n",
    "    if bins_limits is None and col == \"grade\":\n",
    "        bins_limits = [5, 10, 15, 20, 25, 30]\n",
    "        labels = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"]\n",
    "    elif labels is None:\n",
    "        labels = [f\"{col}_{i}\" for i in range(len(bins_limits) - 1)]\n",
    "    \n",
    "    df = df.withColumn(col+\"_discretized\", fn.when(fn.col(col) <= bins_limits[0], labels[0]).otherwise(fn.col(col)))\n",
    "    for i in range(1, len(bins_limits)):\n",
    "        df = df.withColumn(col+\"_discretized\", fn.when(fn.col(col+\"_discretized\") <= bins_limits[i], labels[i]).otherwise(fn.col(col+\"_discretized\")))\n",
    "    df = df.withColumn(col+\"_discretized\", fn.when(fn.col(col+\"_discretized\") > bins_limits[-1], labels[-1]).otherwise(fn.col(col+\"_discretized\")))\n",
    "\n",
    "    return df\n",
    "discretize_column(fintech_df, \"grade\").select(\"grade\", \"grade_discretized\").distinct().show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, apply the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/27 03:25:34 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# - Emp Length: Change to numerical\n",
    "# - Home Ownership: One Hot Encoding\n",
    "# - Verification Status: One Hot Encoding\n",
    "# - State: Label Encoding\n",
    "# - Type: One Hot Encoding\n",
    "# - Purpose: Label Encoding\n",
    "# - For the grade, only descretize it to be letter grade, not need to label encode it further\n",
    "\n",
    "fintech_df = convert_emp_length_to_int(fintech_df)\n",
    "fintech_df = one_hot_encode(fintech_df, [\"home_ownership\", \"verification_status\", \"type\"])\n",
    "fintech_df = label_encode(fintech_df, [\"state\", \"purpose\"])[0]\n",
    "fintech_df = discretize_column(fintech_df, \"grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+----------+------------------+--------------------+-------------------+------------------+--------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+----------+---------------+---------------+-------------+---------------+-----------------+\n",
      "|         customer_id|      emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|      issue_date|pymnt_plan|      type|           purpose|         description|home_ownership_rent|home_ownership_own|home_ownership_other|home_ownership_mortgage|home_ownership_any|verification_status_verified|verification_status_source_verified|verification_status_not_verified|type_joint_app|type_joint|type_individual|type_direct_pay|state_encoded|purpose_encoded|grade_discretized|\n",
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+----------+------------------+--------------------+-------------------+------------------+--------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+----------+---------------+---------------+-------------+---------------+-----------------+\n",
      "|YidceGJiS1x4OGN3X...|Account Manager|         1|      mortgage|   79000.0|             0.0|       not_verified|   950xx|        CA|    55198.0|   551977.0| 182410|    Current|    18475.0|   CA|      18475.0| 36 months|  0.0796|    4|   18 March 2018|     false|individual|       credit_card|Credit card refin...|              false|             false|               false|                   true|             false|                       false|                              false|                            true|         false|     false|           true|          false|           45|             11|                A|\n",
      "|Yic6XHg5ND5ceDEzY...|        Teacher|        10|           own|   71000.0|             0.0|    source_verified|   700xx|        LA|     2120.0|    21195.0|  28829|    Current|     5000.0|   LA|       5000.0| 36 months|  0.1075|    6|16 February 2016|     false|individual|debt_consolidation|  Debt consolidation|              false|              true|               false|                  false|             false|                       false|                               true|                           false|         false|     false|           true|          false|           32|             10|                B|\n",
      "|YiJceDFjXHg4NjZce...| Tax Specialist|        10|          rent|   60000.0|             0.0|    source_verified|   983xx|        WA|     2028.0|    20279.0|  64324|    Current|     8000.0|   WA|       8000.0| 36 months|  0.0949|   10|    16 July 2016|     false|individual|debt_consolidation|  Debt consolidation|               true|             false|               false|                  false|             false|                       false|                               true|                           false|         false|     false|           true|          false|            3|             10|                B|\n",
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------------+----------+----------+------------------+--------------------+-------------------+------------------+--------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+----------+---------------+---------------+-------------+---------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that adds the 3 following features. Try as much as you can to use built in fucntions in PySpark (from the functions library) check lab 8.\n",
    "<br> Avoid writing UDFs from scratch.\n",
    "- Previous loan issue date from the same grade\n",
    "- Previoius Loan amount from the same grade\n",
    "- Previous loan date from the same state and grade combined\n",
    "- Previous loan amount from the same state and grade combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert the issue date to a date format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_issueD_to_date(df : DataFrame) -> DataFrame:\n",
    "    df = df.withColumn(\"issue_date\", fn.to_date(fn.to_timestamp(fn.col(\"issue_date\"), \"d MMMM yyyy\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_new_features(df : DataFrame) -> DataFrame:\n",
    "    #first convert the issue_date to date for proper date sorting\n",
    "    df = convert_issueD_to_date(df)\n",
    "\n",
    "    # Add prv_grade_loan_issue_date\n",
    "    window = Window.partitionBy(\"grade\").orderBy(\"issue_date\")\n",
    "    df = df.withColumn(\"prv_grade_loan_issue_date\", fn.lag(\"issue_date\").over(window))\n",
    "\n",
    "    # Add prv_loan_amnt\n",
    "    #TODO: check if this is correct\n",
    "    df = df.withColumn(\"prv_loan_amount\", fn.lag(\"loan_amount\").over(window))\n",
    "\n",
    "    # Add prv_state_grade_issue_date\n",
    "    window = Window.partitionBy(\"state\", \"grade\").orderBy(\"issue_date\")\n",
    "    df = df.withColumn(\"prv_state_grade_issue_date\", fn.lag(\"issue_date\").over(window))\n",
    "\n",
    "    # Add prv_state_grade_loan_amount\n",
    "    df = df.withColumn(\"prv_state_grade_loan_amount\", fn.lag(\"loan_amount\").over(window))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------------+-----------+---------------+-------------------------+\n",
      "|issue_date|grade|grade_discretized|loan_amount|prv_loan_amount|prv_grade_loan_issue_date|\n",
      "+----------+-----+-----------------+-----------+---------------+-------------------------+\n",
      "|2013-01-13|   30|                F|    20000.0|           null|                     null|\n",
      "|2013-02-13|   30|                F|     5000.0|        20000.0|               2013-01-13|\n",
      "|2013-04-13|   30|                F|    35000.0|         5000.0|               2013-02-13|\n",
      "|2013-05-13|   30|                F|    21600.0|        35000.0|               2013-04-13|\n",
      "|2013-05-13|   30|                F|    28000.0|        21600.0|               2013-05-13|\n",
      "|2013-06-13|   30|                F|    35000.0|        28000.0|               2013-05-13|\n",
      "|2013-06-13|   30|                F|    22000.0|        35000.0|               2013-06-13|\n",
      "|2013-08-13|   30|                F|    20000.0|        22000.0|               2013-06-13|\n",
      "|2013-08-13|   30|                F|    10000.0|        20000.0|               2013-08-13|\n",
      "|2013-08-13|   30|                F|    29000.0|        10000.0|               2013-08-13|\n",
      "+----------+-----+-----------------+-----------+---------------+-------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+--------------------+-------------------+------------------+--------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+----------+---------------+---------------+-------------+---------------+-----------------+-------------------------+---------------+--------------------------+---------------------------+\n",
      "|         customer_id|           emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|issue_date|pymnt_plan|      type|           purpose|         description|home_ownership_rent|home_ownership_own|home_ownership_other|home_ownership_mortgage|home_ownership_any|verification_status_verified|verification_status_source_verified|verification_status_not_verified|type_joint_app|type_joint|type_individual|type_direct_pay|state_encoded|purpose_encoded|grade_discretized|prv_grade_loan_issue_date|prv_loan_amount|prv_state_grade_issue_date|prv_state_grade_loan_amount|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+--------------------+-------------------+------------------+--------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+----------+---------------+---------------+-------------+---------------+-----------------+-------------------------+---------------+--------------------------+---------------------------+\n",
      "|YiJceDkwXHg5N1x4Y...|Heavy Equipment M...|         5|      mortgage|   80000.0|             0.0|       not_verified|   995xx|        AK|    21457.0|   343307.0| 155398|Charged Off|    15000.0|   AK|      15000.0| 60 months|  0.1144|    6|2015-01-15|     false|individual|       credit_card|Credit card refin...|              false|             false|               false|                   true|             false|                       false|                              false|                            true|         false|     false|           true|          false|           49|             11|                B|               2015-01-15|         9000.0|                      null|                       null|\n",
      "|YidceGQ5XHhkNlx4Z...|Human Resource Co...|         2|          rent|   56640.0|             0.0|    source_verified|   998xx|        AK|     8633.0|   120855.0| 164448| Fully Paid|    16000.0|   AK|      16000.0| 36 months|  0.1153|    6|2015-10-15|     false|individual|debt_consolidation|  Debt consolidation|               true|             false|               false|                  false|             false|                       false|                               true|                           false|         false|     false|           true|          false|           49|             10|                B|               2015-10-15|        24000.0|                2015-01-15|                    15000.0|\n",
      "|YidceDE1XHg4Zlx4Z...|               Pilot|         1|          rent|   98600.0|             0.0|    source_verified|   995xx|        AK|     3501.0|    21006.0| 262700|    Current|    36000.0|   AK|      36000.0| 36 months|  0.1198|    6|2018-02-18|     false|individual|             house|         Home buying|               true|             false|               false|                  false|             false|                       false|                               true|                           false|         false|     false|           true|          false|           49|              8|                B|               2018-02-18|        40000.0|                2015-10-15|                    16000.0|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+--------------------+-------------------+------------------+--------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+----------+---------------+---------------+-------------+---------------+-----------------+-------------------------+---------------+--------------------------+---------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df = add_new_features(fintech_df)\n",
    "fintech_df.filter(fn.col(\"grade\") == 30)\\\n",
    "    .select(\"issue_date\", \"grade\", \"grade_discretized\", \"loan_amount\", \"prv_loan_amount\", \"prv_grade_loan_issue_date\")\\\n",
    "        .orderBy(\"issue_date\").show(10)\n",
    "fintech_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Analysis SQL VS Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer each of the following questions using both SQL and Spark:\n",
    "#### 1. Identify the average loan amount and interest rate for loans marked as \"Default\" in the Loan Status, grouped by Emp Length and annual income ranges.<br>\n",
    "Hint: Use SQL Cases to bin Annual Income into Income Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       loan_status|\n",
      "+------------------+\n",
      "|        Fully Paid|\n",
      "|   In Grace Period|\n",
      "|       Charged Off|\n",
      "|Late (31-120 days)|\n",
      "|           Current|\n",
      "| Late (16-30 days)|\n",
      "|           Default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df.select('loan_status').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|       loan_status|count|\n",
      "+------------------+-----+\n",
      "|        Fully Paid| 7622|\n",
      "|   In Grace Period|  159|\n",
      "|       Charged Off| 1727|\n",
      "|Late (31-120 days)|  325|\n",
      "|           Current|17116|\n",
      "| Late (16-30 days)|   79|\n",
      "|           Default|    2|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df.groupBy('loan_status').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_loanamnt_and_intrate_by_emplength_annualinc(df : DataFrame, withSQL: bool = False) -> DataFrame:\n",
    "    # first smallize and fix the loan_status values\n",
    "    df = df.withColumn(\"loan_status\", fn.lower(fn.regexp_replace(fn.regexp_replace(\"loan_status\", \"-\", \"_\"), \" \", \"_\")))\n",
    "    # get the 1st, 2nd, 3rd quartiles of annual_inc\n",
    "    quartiles = df.approxQuantile(\"annual_inc\", [0.25, 0.5, 0.75], 0.01)\n",
    "    print(f\"Quartiles of annual_inc: {quartiles}\")\n",
    "    if not withSQL:\n",
    "        # bin the annual income first\n",
    "        df_inc_discretized = discretize_column(df, \"annual_inc\", bins_limits=quartiles, labels=[\"low\", \"medium\", \"high\", \"very_high\"])\n",
    "\n",
    "        return df_inc_discretized.filter(fn.col(\"loan_status\") == \"default\").groupBy(\"emp_length\", \"annual_inc_discretized\").agg(fn.avg(\"loan_amount\").alias(\"avg_loan_amount\"),fn.avg(\"int_rate\").alias(\"avg_int_rate\"))\n",
    "    # SQL\n",
    "    query = f\"\"\"\n",
    "        SELECT emp_length, \n",
    "            CASE \n",
    "                WHEN annual_inc <= {quartiles[0]} THEN 'low'\n",
    "                WHEN annual_inc <= {quartiles[1]} THEN 'medium'\n",
    "                WHEN annual_inc <= {quartiles[2]} THEN 'high'\n",
    "                ELSE 'very_high'\n",
    "            END AS annual_inc_discretized,\n",
    "            AVG(loan_amount) AS avg_loan_amount,\n",
    "            AVG(int_rate) AS avg_int_rate\n",
    "        FROM fintech\n",
    "        WHERE loan_status = 'default'\n",
    "        GROUP BY emp_length, annual_inc_discretized\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView(\"fintech\")\n",
    "    return spark.sql(query)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quartiles of annual_inc: [47500.0, 66000.0, 93000.0]\n",
      "+----------+----------------------+---------------+------------+\n",
      "|emp_length|annual_inc_discretized|avg_loan_amount|avg_int_rate|\n",
      "+----------+----------------------+---------------+------------+\n",
      "|        10|                   low|        13925.0|      0.2215|\n",
      "|         2|                   low|         4000.0|      0.1899|\n",
      "+----------+----------------------+---------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quartiles of annual_inc: [47500.0, 66000.0, 93000.0]\n",
      "+----------+----------------------+---------------+------------+\n",
      "|emp_length|annual_inc_discretized|avg_loan_amount|avg_int_rate|\n",
      "+----------+----------------------+---------------+------------+\n",
      "|        10|                   low|        13926.0|      0.2217|\n",
      "|         2|                   low|         4500.0|      0.1912|\n",
      "|         2|             very_high|        80123.0|      0.2456|\n",
      "+----------+----------------------+---------------+------------+\n",
      "\n",
      "Using SQL==================================================\n",
      "Quartiles of annual_inc: [47500.0, 66000.0, 93000.0]\n",
      "+----------+----------------------+---------------+------------+\n",
      "|emp_length|annual_inc_discretized|avg_loan_amount|avg_int_rate|\n",
      "+----------+----------------------+---------------+------------+\n",
      "|        10|                   low|        13925.0|      0.2215|\n",
      "|         2|                   low|         4000.0|      0.1899|\n",
      "+----------+----------------------+---------------+------------+\n",
      "\n",
      "Quartiles of annual_inc: [47500.0, 66000.0, 93000.0]\n",
      "+----------+----------------------+---------------+------------+\n",
      "|emp_length|annual_inc_discretized|avg_loan_amount|avg_int_rate|\n",
      "+----------+----------------------+---------------+------------+\n",
      "|        10|                   low|        13926.0|      0.2217|\n",
      "|         2|                   low|         4500.0|      0.1912|\n",
      "|         2|             very_high|        80123.0|      0.2456|\n",
      "+----------+----------------------+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = fintech_df.select(\"emp_length\", \"annual_inc\", \"loan_amount\", \"int_rate\", \"loan_status\")\\\n",
    "    .union(spark.createDataFrame([(\"10\", 40000, 13927.0, 0.2219, \"default\"), \\\n",
    "                                  (\"2\", 10000, 5000.0, 0.1925, \"default\"),\\\n",
    "                                  (\"2\", 95000, 80123.0, 0.2456, \"default\")], \\\n",
    "            [\"emp_length\", \"annual_inc\", \"loan_amount\", \"int_rate\", \"loan_status\"]))\n",
    "\n",
    "avg_loanamnt_and_intrate_by_emplength_annualinc(fintech_df).show()\n",
    "avg_loanamnt_and_intrate_by_emplength_annualinc(test_df).show()\n",
    "\n",
    "print(\"Using SQL\" + \"=\"*50)\n",
    "\n",
    "avg_loanamnt_and_intrate_by_emplength_annualinc(fintech_df, withSQL=True).show()\n",
    "avg_loanamnt_and_intrate_by_emplength_annualinc(test_df, withSQL=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Calculate the average difference between Loan Amount and Funded Amount for each loan Grade and sort by the grades with the largest differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_diff_loan_funded(df : DataFrame, withSQL: bool = False) -> DataFrame:\n",
    "    if not withSQL:\n",
    "        return df.withColumn(\"diff_loan_funded\", fn.col(\"loan_amount\") - fn.col(\"funded_amount\"))\\\n",
    "            .groupBy(\"grade\").agg(fn.avg(\"diff_loan_funded\").alias(\"avg_diff_loan_funded\"))\\\n",
    "                .orderBy(\"avg_diff_loan_funded\", ascending=False)\n",
    "    # SQL\n",
    "    print(\"SQL\")\n",
    "    query = \"\"\"\n",
    "        SELECT grade,\n",
    "        AVG(diff_loan_funded) AS avg_diff_loan_funded\n",
    "        FROM \n",
    "        (\n",
    "            SELECT loan_amount, funded_amount, grade,\n",
    "                (loan_amount - funded_amount)   AS  diff_loan_funded\n",
    "            FROM fintech\n",
    "        )\n",
    "        GROUP BY grade\n",
    "        ORDER BY avg_diff_loan_funded DESC\n",
    "        \"\"\"\n",
    "    df.createOrReplaceTempView(\"fintech\")\n",
    "    return spark.sql(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|grade|avg_diff_loan_funded|\n",
      "+-----+--------------------+\n",
      "|   29|                 0.0|\n",
      "|   26|                 0.0|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|grade|avg_diff_loan_funded|\n",
      "+-----+--------------------+\n",
      "|   36|               500.0|\n",
      "|   30|  10.989010989010989|\n",
      "|   29|                 0.0|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "SQL\n",
      "+-----+--------------------+\n",
      "|grade|avg_diff_loan_funded|\n",
      "+-----+--------------------+\n",
      "|   36|               500.0|\n",
      "|   30|  10.989010989010989|\n",
      "|   29|                 0.0|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "SQL\n",
      "+-----+--------------------+\n",
      "|grade|avg_diff_loan_funded|\n",
      "+-----+--------------------+\n",
      "|   29|                 0.0|\n",
      "|   26|                 0.0|\n",
      "+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_diff_loan_funded(fintech_df).show(2)\n",
    "test_df = fintech_df.select(\"loan_amount\", \"funded_amount\", \"grade\")\\\n",
    "        .union(spark.createDataFrame([(10000, 9500, 36), (20000, 19000, 30)], [\"loan_amount\", \"funded_amount\", \"grade\"]))\n",
    "avg_diff_loan_funded(test_df).show(3)\n",
    "\n",
    "# print(\"SQL\"+(\"=\"*50))\n",
    "avg_diff_loan_funded(test_df,withSQL=True).show(3)\n",
    "avg_diff_loan_funded(fintech_df, withSQL=True).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compare the total Loan Amount for loans with \"Verified\" and \"Not Verified\" Verification Status across each state (Addr State)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_loan_amount_by_state(df : DataFrame, withSQL: bool = False) -> DataFrame:\n",
    "    # first smallize and fix the loan_status values\n",
    "    df = df.withColumn(\"verification_status\", fn.lower(fn.regexp_replace(fn.regexp_replace(\"verification_status\", \"-\", \"_\"), \" \", \"_\")))\n",
    "    if not withSQL:\n",
    "        return df.filter((fn.col('verification_status') == \"verified\") | (fn.col('verification_status') == \"not_verified\"))\\\n",
    "                .groupBy(\"addr_state\").agg(fn.sum(\"loan_amount\").alias(\"state_sum_loan_amount\"))\n",
    "    #SQL\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            addr_state, SUM(loan_amount)\n",
    "        from fintech\n",
    "        WHERE verification_status = 'verified' OR\n",
    "          verification_status = 'not_verified'\n",
    "        GROUP BY addr_state\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df.createOrReplaceTempView(\"fintech\")\n",
    "    return spark.sql(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------+\n",
      "|addr_state|verification_status|loan_amount|\n",
      "+----------+-------------------+-----------+\n",
      "|        NY|       not_verified|     3500.0|\n",
      "|        CA|       not_verified|    24000.0|\n",
      "|        FL|           verified|    10000.0|\n",
      "|        FL|       not_verified|    24000.0|\n",
      "|        AZ|       not_verified|    10000.0|\n",
      "|        GA|       not_verified|     5000.0|\n",
      "|        NJ|           verified|    25000.0|\n",
      "|        NJ|       not_verified|    18000.0|\n",
      "|        NY|           verified|    10000.0|\n",
      "|        MI|           verified|     7800.0|\n",
      "+----------+-------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fintech_df.filter((fn.lower(fn.col('verification_status')) == \"verified\") | (fn.lower(fn.col('verification_status')) == \"not_verified\"))\\\n",
    ".select(\"addr_state\", \"verification_status\", \"loan_amount\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+\n",
      "|addr_state|state_sum_loan_amount|\n",
      "+----------+---------------------+\n",
      "|        SC|            3193600.0|\n",
      "|        AZ|            5617075.0|\n",
      "|        LA|            2899775.0|\n",
      "|        MN|            4632075.0|\n",
      "|        NJ|          1.0058625E7|\n",
      "+----------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "SQL==================================================\n",
      "+----------+----------------+\n",
      "|addr_state|sum(loan_amount)|\n",
      "+----------+----------------+\n",
      "|        SC|       3193600.0|\n",
      "|        AZ|       5617075.0|\n",
      "|        LA|       2899775.0|\n",
      "|        MN|       4632075.0|\n",
      "|        NJ|     1.0058625E7|\n",
      "+----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_loan_amount_by_state(fintech_df).show(5)\n",
    "\n",
    "print(\"SQL\"+(\"=\"*50))\n",
    "\n",
    "sum_loan_amount_by_state(fintech_df, withSQL=True).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Calculate the average time gap (in days) between consecutive loans for each grade using the new features you added in the feature engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_days_gap_for_grade(df : DataFrame, withSQL: bool = False) -> DataFrame:\n",
    "    # make sure issue date is date\n",
    "    df = convert_issueD_to_date(df)\n",
    "    if not withSQL:\n",
    "        return df.withColumn('days_gap', fn.datediff(fn.col('issue_date'), fn.col('prv_grade_loan_issue_date')))\\\n",
    "            .groupBy('grade').agg(fn.avg('days_gap').alias('avg_days_gap'))\n",
    "    #SQL\n",
    "    query =\"\"\"\n",
    "        SELECT \n",
    "            grade,\n",
    "            AVG(days_gap) AS avg_days_gap\n",
    "        FROM (\n",
    "            SELECT grade, DATEDIFF(DAY, prv_grade_loan_issue_date, issue_date) AS days_gap \n",
    "            FROM fintech\n",
    "        )\n",
    "        GROUP BY grade\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView(\"fintech\")\n",
    "    return spark.sql(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|grade|      avg_days_gap|\n",
      "+-----+------------------+\n",
      "|    1|2.3138622493461205|\n",
      "|    2| 2.270316509837468|\n",
      "|    3|2.2742073693230505|\n",
      "|    4|2.2988013698630136|\n",
      "|    5| 2.287931034482759|\n",
      "|    6|1.6733921815889028|\n",
      "+-----+------------------+\n",
      "only showing top 6 rows\n",
      "\n",
      "+-----+------------------+\n",
      "|grade|      avg_days_gap|\n",
      "+-----+------------------+\n",
      "|    1|2.3138622493461205|\n",
      "|    2| 2.270316509837468|\n",
      "|    3|2.2742073693230505|\n",
      "|    4|2.2988013698630136|\n",
      "|    5| 2.287931034482759|\n",
      "|    6|1.6733921815889028|\n",
      "+-----+------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_days_gap_for_grade(fintech_df).show(6)\n",
    "avg_days_gap_for_grade(fintech_df, withSQL=True).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. Identify the average difference in loan amounts between consecutive loans within the same state and grade combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_diff_consecutive_loan_amount(df : DataFrame, withSQL: bool = False) -> DataFrame:\n",
    "    # make sure issue date is date\n",
    "    df = convert_issueD_to_date(df)\n",
    "    if not withSQL:\n",
    "        return df.withColumn('loans_diff', fn.col('loan_amount') - fn.col('prv_state_grade_loan_amount'))\\\n",
    "            .groupBy('grade', 'state').agg(fn.avg('loans_diff').alias('avg_diff_loan_amount'))\n",
    "    #SQL\n",
    "    query =\"\"\"\n",
    "        SELECT \n",
    "            grade, state,\n",
    "            AVG(loans_diff) AS avg_diff_loan_amount\n",
    "        FROM (\n",
    "            SELECT grade, state, (loan_amount - prv_state_grade_loan_amount) AS loans_diff \n",
    "            FROM fintech\n",
    "        )\n",
    "        GROUP BY grade, state\n",
    "    \"\"\"\n",
    "    df.createOrReplaceTempView(\"fintech\")\n",
    "    return spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+\n",
      "|grade|state|avg_diff_loan_amount|\n",
      "+-----+-----+--------------------+\n",
      "|    3|   AL| -133.33333333333334|\n",
      "|    7|   AL|   488.8888888888889|\n",
      "|   22|   AL|             -1795.0|\n",
      "|   26|   AL|                null|\n",
      "|   33|   AL|                null|\n",
      "|    2|   AR|  1333.3333333333333|\n",
      "|   22|   AR|                null|\n",
      "|   11|   AZ|  -92.44186046511628|\n",
      "+-----+-----+--------------------+\n",
      "only showing top 8 rows\n",
      "\n",
      "SQL==================================================\n",
      "+-----+-----+--------------------+\n",
      "|grade|state|avg_diff_loan_amount|\n",
      "+-----+-----+--------------------+\n",
      "|    9|   AL|               200.0|\n",
      "|   12|   AL|  180.35714285714286|\n",
      "|   24|   AR|                null|\n",
      "|    1|   AZ|   785.7142857142857|\n",
      "|    7|   AZ|   270.5882352941176|\n",
      "|   10|   AZ| -211.53846153846155|\n",
      "|    2|   CA| -27.027027027027028|\n",
      "|   25|   CA|  281.57894736842104|\n",
      "+-----+-----+--------------------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_diff_consecutive_loan_amount(fintech_df).sample(0.1).show(8)\n",
    "print(\"SQL\"+(\"=\"*50))\n",
    "avg_diff_consecutive_loan_amount(fintech_df, withSQL=True).sample(0.1).show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Lookup Table & Saving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.1: Lookup Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a lookup table for the encodings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "|column|original|imputed|\n",
      "+------+--------+-------+\n",
      "| state|      IL|     36|\n",
      "| state|      HI|     38|\n",
      "| grade|       9|      B|\n",
      "| grade|      20|      D|\n",
      "| grade|      23|      E|\n",
      "| grade|      26|      F|\n",
      "| grade|      33|      G|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label encoded columns are: state, purpose\n",
    "def populate_lookup_table(df : DataFrame, cols : list) -> DataFrame:\n",
    "    global Lookup_Table\n",
    "\n",
    "    for col in cols:\n",
    "        suffix = \"_encoded\" if col in [\"state\", \"purpose\"] else \"_discretized\"\n",
    "        df_targets = df.select(col, f\"{col}{suffix}\" ).dropDuplicates(subset=[col, f\"{col}{suffix}\"]).orderBy(f'{col}{suffix}')\n",
    "        df_targets = df_targets.withColumn(\"column\", fn.lit(col))\n",
    "        df_targets = df_targets.withColumnRenamed(col, \"original\")\n",
    "        df_targets = df_targets.withColumnRenamed(f\"{col}{suffix}\", \"imputed\")\n",
    "        Lookup_Table = Lookup_Table.union(df_targets.select(\"column\", \"original\", \"imputed\"))\n",
    "    return Lookup_Table\n",
    "\n",
    "populate_lookup_table(fintech_df, [\"state\", \"purpose\", \"grade\"]).sample(0.1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.2: Saving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally load (save) the cleaned PySpark df and the lookup table to parquet\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the lookup table\n",
    "Lookup_Table.write.mode(\"overwrite\").parquet(data_dir + LOOKUP_FILE)\n",
    "\n",
    "# save the pySpark dataframe\n",
    "fintech_df.write.mode(\"overwrite\").parquet(data_dir + CLEAN_DATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fintech_df.write.mode(\"overwrite\").csv(data_dir + \"fintech_spark_52_20136_clean.csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Bonus - Loading to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the cleaned parquet file and lookup table into a Postgres database.\n",
    "- Take Screenshots showing the newly added features in the feature engineering section\n",
    "- Take a screenshot from the lookup table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yx5cIsxRQIbBwOw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from supabase import create_client, Client\n",
    "\n",
    "url: str = 'https://mdhrcvfeajsufjjbkwmd.supabase.co'\n",
    "key: str = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im1kaHJjdmZlYWpzdWZqamJrd21kIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzI2Njk5MTMsImV4cCI6MjA0ODI0NTkxM30.-xOuYxJ8Xi84vL2BYMUq5Rt16791XIcJa7ijy2pVTk4'\n",
    "supabase: Client = create_client(url, key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "1. Python Notebook with the following naming m3_spark_<id>.ipynb eg.\n",
    "m3_spark_52_XXXX.ipynb\n",
    "2. Cleaned Parquet file named: fintech_spark_52_XXXX_clean.parquet\n",
    "3. Lookup table named: lookup_spark_52_XXXX.parquet\n",
    "4. Incase of doing the bonus: Screenshots from PGAdmin showing the cleaned table\n",
    "(some of the rows) and another one showing the lookup table. <br>\n",
    "> Note: All these files should reside in a folder for milestone 3, inside the root drive folder created previously in milestone 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission guidelines\n",
    "Upload all the deliverables in your google drive milestone folder.\n",
    "Best of luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing Spark Session Context\n",
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DE Env",
   "language": "python",
   "name": "deenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
