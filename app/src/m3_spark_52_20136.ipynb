{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3 - PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 14px;\">\n",
    "By:\n",
    "\n",
    "- Mohamed Ayman Mohamed Mohamed abo Tammaa\n",
    "    - 52-20136\n",
    "    - mohamed.abotammaa@student.guc.edu.eg\n",
    "    - P02\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "1. Loading the dataset (5%)\n",
    "2. Perform some simple cleaning (30%)\n",
    "    - Column renaming: 10%\n",
    "    - Detect missing: 35%\n",
    "    - Handle missing: 35%\n",
    "    - Check missing : 20%\n",
    "3. Perform some analysis on the dataset (30%)\n",
    "4. Add new columns with feature engineering (15%)\n",
    "5. Encode categorical columns (10%) \n",
    "6. Create a lookup table for encoding only (5%)\n",
    "7. Saving Cleaned dataseta and lookup table (5%)\n",
    "8. ***BONUS**: Saving the output into a postgres database (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that:** You may not need to run the spark containers since pyspark aleady\n",
    "creates a mini server by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0: Libraries & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"m3_spark\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../Datasets/\"\n",
    "ORIGINAL_DATAFILE = \"fintech_data_38_52_20136.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply load the dataset from the parquet format given in the google drive above\n",
    "- Load the dataset.\n",
    "- Preview first 20 rows.\n",
    "- How many partitions is this dataframe split into?\n",
    "- Change partitions to be equal to the number of your logical cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rename all columns (replacing a space with an underscore, and making it lowercase)\n",
    "- Detect missing\n",
    "    - Create a function that takes in the df and returns any data structrue of your choice(df/dict,list,tuple,etc) which has the name of the column and percentage of missing entries from the whole dataset.\n",
    "    - Tip : storing the missing info as dict where the key is the column name and value is the percentage would be the easiest.\n",
    "- Prinout the missing info\n",
    "- Handle missing\n",
    "    - For numerical features replace with 0.\n",
    "    - For categorical/strings replace with mode\n",
    "- Check missing\n",
    "    - Afterwards, check that there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode only the following categorical values\n",
    "- Emp Length: Change to numerical\n",
    "- Home Ownership: One Hot Encoding\n",
    "- Verification Status: One Hot Encoding\n",
    "- State: Label Encoding\n",
    "- Type: One Hot Encoding\n",
    "- Purpose: Label Encoding\n",
    "- For the grade, only descretize it to be letter grade, not need to label encode it further\n",
    "\n",
    "**DO NOT** Encode the employment title of description or any other column that is not mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that adds the 3 following features. Try as much as you can to use built in fucntions in PySpark (from the functions library) check lab 8.\n",
    "<br> Avoid writing UDFs from scratch.\n",
    "- Previous loan issue date from the same grade\n",
    "- Previoius Loan amount from the same grade\n",
    "- Previous loan date from the same state and grade combined\n",
    "- Previous loan amount from the same state and grade combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Analysis SQL VS Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer each of the following questions using both SQL and Spark:\n",
    "1. Identify the average loan amount and interest rate for loans marked as \"Default\" in the Loan Status, grouped by Emp Length and annual income ranges.<br>\n",
    "Hint: Use SQL Cases to bin Annual Income into Income Ranges\n",
    "2. Calculate the average difference between Loan Amount and Funded Amount for each\n",
    "loan Grade and sort by the grades with the largest differences.\n",
    "3. Compare the total Loan Amount for loans with \"Verified\" and \"Not Verified\"\n",
    "Verification Status across each state (Addr State).\n",
    "4. Calculate the average time gap (in days) between consecutive loans for each\n",
    "grade using the new features you added in the feature engineering phase.\n",
    "5. Identify the average difference in loan amounts between consecutive loans\n",
    "within the same state and grade combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Lookup Table & Saving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.1: Lookup Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a lookup table for the encodings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.2: Saving the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally load (save) the cleaned PySpark df and the lookup table to parquet\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Bonus - Loading to Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load the cleaned parquet file and lookup table into a Postgres database.\n",
    "- Take Screenshots showing the newly added features in the feature engineering section\n",
    "- Take a screenshot from the lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables\n",
    "1. Python Notebook with the following naming m3_spark_<id>.ipynb eg.\n",
    "m3_spark_52_XXXX.ipynb\n",
    "2. Cleaned Parquet file named: fintech_spark_52_XXXX_clean.parquet\n",
    "3. Lookup table named: lookup_spark_52_XXXX.parquet\n",
    "4. Incase of doing the bonus: Screenshots from PGAdmin showing the cleaned table\n",
    "(some of the rows) and another one showing the lookup table.\n",
    "Note: All these files should reside in a folder for milestone 3, inside the root drive\n",
    "folder created previously in milestone 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission guidelines\n",
    "Upload all the deliverables in your google drive milestone folder.\n",
    "Best of luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing Spark Session Context\n",
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DE Env",
   "language": "python",
   "name": "deenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
